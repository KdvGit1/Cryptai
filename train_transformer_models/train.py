import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import time
import os

# Motor dosyamÄ±zdan gerekli parÃ§alarÄ± alÄ±yoruz
from ai_engine import CryptoDataset, CryptoTransformer

# =========================================================
# EÄÄ°TÄ°M FONKSÄ°YONU
# =========================================================
def train_specific_model(coin_name, timeframe, month_period):
    """
    Target Scaling ve Huber Loss ile gÃ¼Ã§lendirilmiÅŸ eÄŸitim fonksiyonu.
    """

    # Dosya Ä°simleri
    csv_path = f"{coin_name}_{month_period}Ay_{timeframe}_AI_Ready.csv"
    model_save_name = f"{coin_name}_{month_period}Ay_{timeframe}_MODEL.pth"

    print(f"\n{'='*60}")
    print(f"ğŸ¯ HEDEF (SCALED): {coin_name} | {timeframe} | {month_period} AylÄ±k Veri")
    print(f"ğŸ“‚ Okunacak: {csv_path}")
    print(f"{'='*60}")

    if not os.path.exists(csv_path):
        print(f"âŒ HATA: {csv_path} bulunamadÄ±! Ã–nce veri Ã§ekmelisin.")
        return

    # --- AYARLAR ---
    CONFIG = {
        'seq_len': 120,
        'batch_size': 256,      # BÃ¼yÃ¼k batch (RTX 3060 iÃ§in uygun)
        'd_model': 256,
        'nhead': 8,
        'num_layers': 4,
        'epochs': 50,
        'learning_rate': 0.0005 # Huber Loss ile biraz daha dÃ¼ÅŸÃ¼k LR iyidir
    }

    # Cihaz SeÃ§imi
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”¥ DonanÄ±m: {device} (GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'Yok'})")

    # Veri Setini YÃ¼kle
    full_dataset = CryptoDataset(csv_path, seq_len=CONFIG['seq_len'])

    # %80 EÄŸitim, %20 Test
    train_size = int(0.8 * len(full_dataset))
    test_size = len(full_dataset) - train_size
    train_data, test_data = random_split(full_dataset, [train_size, test_size])

    # DataLoader (CPU worker sayÄ±sÄ±nÄ± artÄ±rarak veri akÄ±ÅŸÄ±nÄ± hÄ±zlandÄ±rÄ±yoruz)
    # Windows'ta bazen num_workers hata verebilir, verirse 0 yapÄ±n.
    train_loader = DataLoader(train_data, batch_size=CONFIG['batch_size'], shuffle=True, drop_last=True)
    test_loader = DataLoader(test_data, batch_size=CONFIG['batch_size'], shuffle=False, drop_last=True)

    # Modeli Ä°nÅŸa Et
    model = CryptoTransformer(
        input_dim=14, # 14 Ã¶zellik
        d_model=CONFIG['d_model'],
        nhead=CONFIG['nhead'],
        num_layers=CONFIG['num_layers'],
        dropout=0.2
    ).to(device)

    if torch.cuda.device_count() > 1:
        model = nn.DataParallel(model)

    # --- 1. DEÄÄ°ÅÄ°KLÄ°K: HUBER LOSS ---
    # Delta=1.0: Hata 1.0'dan kÃ¼Ã§Ã¼kse karesini al (Hassas), bÃ¼yÃ¼kse dÃ¼z al (Spike korumasÄ±)
    criterion = nn.HuberLoss(delta=1.0)

    optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])

    # --- YENÄ° EKLENTÄ°: SCHEDULER ---
    # EÄŸer "Test Loss" 5 epoch boyunca dÃ¼ÅŸmezse, Ã¶ÄŸrenme hÄ±zÄ±nÄ± (LR) yarÄ±ya indir.
    # Bu, tÄ±kanan modelin kilidini aÃ§ar.
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)

    # EÄÄ°TÄ°M DÃ–NGÃœSÃœ
    best_test_loss = float('inf')

    for epoch in range(CONFIG['epochs']):
        start_time = time.time()

        # --- TRAIN ---
        model.train()
        train_loss = 0

        for bx, by in train_loader:
            bx, by = bx.to(device), by.to(device)

            optimizer.zero_grad()
            output = model(bx)

            # --- 2. DEÄÄ°ÅÄ°KLÄ°K: TARGET SCALING ---
            # GerÃ§ek deÄŸeri 100 ile Ã§arpÄ±yoruz.
            # 0.001 -> 0.1 olur. Model bunu daha rahat Ã¶ÄŸrenir.
            scaled_target = by * 100.0

            # Model Ã§Ä±ktÄ±sÄ±nÄ± [Batch, 1] formatÄ±ndan [Batch] formatÄ±na getir (squeeze)
            loss = criterion(output.squeeze(), scaled_target)

            loss.backward()

            # Gradyan patlamasÄ±nÄ± Ã¶nle (GÃ¼venlik sigortasÄ±)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()
            train_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)

        # --- TEST ---
        model.eval()
        test_loss = 0
        with torch.no_grad():
            for bx, by in test_loader:
                bx, by = bx.to(device), by.to(device)

                output = model(bx)

                # Test ederken de aynÄ± Ã¶lÃ§eÄŸi kullanmalÄ±yÄ±z ki adil olsun
                scaled_target = by * 100.0

                loss = criterion(output.squeeze(), scaled_target)
                test_loss += loss.item()

        avg_test_loss = test_loss / len(test_loader)

        # Scheduler'a rapor ver: "Durum bu, gerekirse hÄ±zÄ± dÃ¼ÅŸÃ¼r"
        scheduler.step(avg_test_loss)

        # --- KAYIT ---
        elapsed = time.time() - start_time
        save_msg = ""

        if avg_test_loss < best_test_loss:
            best_test_loss = avg_test_loss
            torch.save(model.state_dict(), model_save_name)
            save_msg = "âœ… REKOR & KAYIT"

        # Mevcut Ã¶ÄŸrenme hÄ±zÄ±nÄ± al
        current_lr = optimizer.param_groups[0]['lr']
        # Loss deÄŸerlerini terminalde 5 haneli gÃ¶sterelim
        print(f"Epoch {epoch+1:02d} | Train: {avg_train_loss:.5f} | Test: {avg_test_loss:.5f} | LR: {current_lr:.6f} | {save_msg}")

    print(f"ğŸ {coin_name} - {timeframe} tamamlandÄ±. En iyi Scaled Loss: {best_test_loss:.5f}")
    print("-" * 60)


# =========================================================
# ANA Ã‡ALIÅTIRICI
# =========================================================
def run_all_trainings():
    coin = "BTC"

    # EÄŸer 6 aylÄ±k veri Ã§ektiysen:
    train_specific_model(coin, "5m", 60)
    train_specific_model(coin, "15m", 60)
    train_specific_model(coin, "1h", 60)

if __name__ == "__main__":
    run_all_trainings()